{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3AoYgPHV5W7tm13wEJyfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nauraift/TextPreprocessing/blob/main/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwkvMWf5zKAM"
      },
      "outputs": [],
      "source": [
        "pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install swifter"
      ],
      "metadata": {
        "id": "6-N4sFAkEChe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Y95QzbSsEKP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PREPROCESSING**"
      ],
      "metadata": {
        "id": "WPjMbUjaET10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FKoSGc6gEcX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tweepy as tweet\n",
        "import string\n",
        "import re \n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "TWEET_DATA = pd.read_csv(\"/content/drive/MyDrive/Skripsi/data_penelitian/copy_of_data.csv\")\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet']\n",
        "\n",
        "\n",
        "\n",
        "print(TWEET_DATA['tweet'].head(10))"
      ],
      "metadata": {
        "id": "mMzCXgQOEed1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Cleansing**"
      ],
      "metadata": {
        "id": "__HWe6F_nPNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tweet_special(text):\n",
        "  #remove tab, new line, ans back slice\n",
        "  text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('u\\\\',\" \").replace('\\\\',\" \")\n",
        "  # remove non ASCII (emoticon, chinese word, .etc)\n",
        "  text = text.encode('ascii', 'replace').decode('ascii')\n",
        "  # remove mention, link, hashtag\n",
        "  text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "  # remove incomplete URL\n",
        "  return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "                \n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_tweet_special)\n",
        "\n",
        "#remove number\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_number)\n",
        "\n",
        "#remove punctuation(tanda baca)\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_punctuation)\n",
        "\n",
        "#remove whitespace leading & trailing\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_whitespace_LT)\n",
        "\n",
        "#remove multiple whitespace into single whitespace\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_whitespace_multiple)\n",
        "\n",
        "# remove single char\n",
        "def remove_singl_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "\n",
        "TWEET_DATA['cleansing'] = TWEET_DATA['tweet'].apply(remove_singl_char)\n",
        "\n",
        "print('Cleansing Result : \\n') \n",
        "print(TWEET_DATA['cleansing'].head())"
      ],
      "metadata": {
        "id": "KNs_n3aoEhUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Case Folding**"
      ],
      "metadata": {
        "id": "NdH6466jnWyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TWEET_DATA['casefold'] = TWEET_DATA['cleansing'].str.lower()\n",
        "\n",
        "print('Case Folding Result : \\n')\n",
        "print(TWEET_DATA['casefold'].head(5))"
      ],
      "metadata": {
        "id": "185wPlKHnjKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tokenizing**"
      ],
      "metadata": {
        "id": "FuekGnNSnmyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# NLTK word tokenize \n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "TWEET_DATA['tokens'] = TWEET_DATA['casefold'].apply(word_tokenize_wrapper)\n",
        "\n",
        "print('Tokenizing Result : \\n') \n",
        "TWEET_DATA['tokens'].head()\n"
      ],
      "metadata": {
        "id": "6X5xZ3_Wnlmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Document Frequency**"
      ],
      "metadata": {
        "id": "03uCZW1on2th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_DF(tfDict):\n",
        "    count_DF = {}\n",
        "    # Run through each document's tf dictionary and increment countDict's (term, doc) pair\n",
        "    for document in tfDict:\n",
        "        for term in document:\n",
        "            if term in count_DF:\n",
        "                count_DF[term] += 1\n",
        "            else:\n",
        "                count_DF[term] = 1\n",
        "    return count_DF\n",
        "\n",
        "df = calc_DF(TWEET_DATA[\"tokens\"])\n",
        "df\n"
      ],
      "metadata": {
        "id": "EPnPsyNYnsVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict([df.keys(),df.values()]).transpose()\n",
        "df.columns= ['word', 'frekuensi']\n",
        "df.sort_values(['frekuensi'],ignore_index=True, inplace=True)\n",
        "df\n",
        "\n",
        "df.to_csv(\"documentfrequency.csv\")"
      ],
      "metadata": {
        "id": "nxw-sUR-oAXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Stopword Removal**"
      ],
      "metadata": {
        "id": "rmrMpnavonrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# get stopword indonesia\n",
        "\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "print(list_stopwords)"
      ],
      "metadata": {
        "id": "t3uvkCQiotqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_stopwords.extend([\"gue\", \"loe\"])\n",
        "\n",
        "txt_stopword = pd.read_csv(\"stopwordlist_bismillah.txt\", names= [\"stopwordlist_bismillah\"], header = None)\n",
        "                  \n",
        "list_stopwords.extend(txt_stopword[\"stopwordlist_bismillah\"])\n",
        "\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "list_stopwords\n"
      ],
      "metadata": {
        "id": "WfZ5_X_vo2mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopword pada list \n",
        "def stopwords_removal(text):\n",
        "    return [text for text in text if text not in list_stopwords]\n",
        "\n",
        "TWEET_DATA['stopword'] = TWEET_DATA['tokens'].apply(stopwords_removal) \n",
        "\n",
        "TWEET_DATA['stopword']"
      ],
      "metadata": {
        "id": "KWqwOfaMo9pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Stemming**"
      ],
      "metadata": {
        "id": "SM3dQqCOo_27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import swifter\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "# stemmed\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "for document in TWEET_DATA['stopword']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "print(len(term_dict))\n",
        "print(\"------------------------\")\n",
        "\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "    print(term,\":\" ,term_dict[term])\n",
        "print(term_dict)\n",
        "print(\"------------------------\")\n",
        "\n",
        "# apply stemmed term to dataframe\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "TWEET_DATA['stemmed'] = TWEET_DATA['stopword'].swifter.apply(get_stemmed_term)\n",
        "TWEET_DATA['stemmed']"
      ],
      "metadata": {
        "id": "HjzWYM5kpC4M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}